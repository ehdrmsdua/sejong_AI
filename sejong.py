# -*- coding: utf-8 -*-
"""sejong.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MzXv4ErXJV1AWQNuVaoe2X7ONnELaVPu
"""

!pip install segmentation_models_pytorch

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler
from torchvision import transforms
from PIL import Image
import os
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import random
import segmentation_models_pytorch as smp

# 모델 정의(deeplabv3)
def create_deeplabv3plus(num_classes):
    model = smp.DeepLabV3Plus(encoder_name="resnet101", classes=num_classes, encoder_weights="imagenet")
    return model

# 데이터셋 클래스 정의
class SegmentationDataset(Dataset):
    def __init__(self, image_dir, label_dir, image_list, transform=None):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.transform = transform
        self.images = image_list

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.images[idx])
        label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '_mask.png'))
        image = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')
        if self.transform:
            image = self.transform(image)
            label = self.transform(label) * 255
        return image, label.squeeze().long()

# 데이터 분할 함수
def split_dataset(image_dir, test_ratio=0.2):
    images = os.listdir(image_dir)
    random.seed(36)
    random.shuffle(images)
    split_idx = int(len(images) * (1 - test_ratio))
    return images[:split_idx], images[split_idx:]

# 데이터 변환 정의
train_transforms = transforms.Compose([
    transforms.Resize((640, 368)),
    transforms.ToTensor(),
])

test_transforms = transforms.Compose([
    transforms.Resize((640, 368)),
    transforms.ToTensor(),
])

# 데이터셋 경로 설정
image_dir = '/content/drive/MyDrive/횡단보도/segmentation/images'
label_dir = '/content/drive/MyDrive/횡단보도/segmentation/labels'

# 데이터셋 분할
train_images, test_images = split_dataset(image_dir)
train_dataset = SegmentationDataset(image_dir, label_dir, train_images, transform=train_transforms)
test_dataset = SegmentationDataset(image_dir, label_dir, test_images, transform=test_transforms)

# DataLoader 설정
test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False)

# 5폴드 교차검증 설정
k_folds = 5
kfold = KFold(n_splits=k_folds, shuffle=True)

results = []

# 모델 저장 경로 설정
model_save_path = '/content/drive/MyDrive/횡단보도/segmentation/models'
if not os.path.exists(model_save_path):
    os.makedirs(model_save_path)

for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):
    print(f'Starting fold {fold}')
    train_subsampler = SubsetRandomSampler(train_ids)
    val_subsampler = SubsetRandomSampler(val_ids)

    train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=train_subsampler)
    val_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=val_subsampler)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = create_deeplabv3plus(num_classes=4).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    num_epochs = 50

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # 검증 세트에서 모델 평가
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

        # 여기서 검증 손실을 results 리스트에 추가
        val_loss /= len(val_loader)
        results.append(val_loss)
        print(f'Fold {fold}, Epoch {epoch+1}, Training Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss}')

        # 모델 저장
        torch.save(model.state_dict(), os.path.join(model_save_path, f'model_fold{fold}_epoch{epoch+1}.pth'))

# 평균 K-Fold 검증 손실 계산 및 출력
average_val_loss = np.mean(results)
print(f'Average K-Fold Validation Loss: {average_val_loss}')

!pip install segmentation_models_pytorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler
from torchvision import transforms
from PIL import Image
import os
import numpy as np
from sklearn.model_selection import KFold
import random
import segmentation_models_pytorch as smp

def create_deeplabv3plus(num_classes):
    model = smp.DeepLabV3Plus(encoder_name="resnet101", classes=num_classes, encoder_weights="imagenet")
    return model

class SegmentationDataset(Dataset):
    def __init__(self, image_dir, label_dir, image_list, transform=None):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.transform = transform
        self.images = image_list

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.images[idx])
        label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '_mask.png'))
        image = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')
        if self.transform:
            image = self.transform(image)
            label = self.transform(label) * 255
        return image, label.squeeze().long()

def split_dataset(image_dir, test_ratio=0.2):
    images = os.listdir(image_dir)
    random.seed(36)
    random.shuffle(images)
    split_idx = int(len(images) * (1 - test_ratio))
    return images[:split_idx], images[split_idx:]

train_transforms = transforms.Compose([
    transforms.Resize((640, 368)),
    transforms.ToTensor(),
])

test_transforms = transforms.Compose([
    transforms.Resize((640, 368)),
    transforms.ToTensor(),
])

image_dir = '/content/drive/MyDrive/횡단보도/segmentation/images'
label_dir = '/content/drive/MyDrive/횡단보도/segmentation/labels'

train_images, test_images = split_dataset(image_dir)
train_dataset = SegmentationDataset(image_dir, label_dir, train_images, transform=train_transforms)
test_dataset = SegmentationDataset(image_dir, label_dir, test_images, transform=test_transforms)

test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False)

k_folds = 5
kfold = KFold(n_splits=k_folds, shuffle=True)

results = []
model_save_path = '/content/drive/MyDrive/횡단보도/4models'
if not os.path.exists(model_save_path):
    os.makedirs(model_save_path)

# fold 4에 대해서만 학습
selected_fold = 4
for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):
    if fold == selected_fold:
        print(f'Starting fold {fold}')
        train_subsampler = SubsetRandomSampler(train_ids)
        val_subsampler = SubsetRandomSampler(val_ids)

        train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=train_subsampler)
        val_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=val_subsampler)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = create_deeplabv3plus(num_classes=4).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        num_epochs = 100

        for epoch in range(num_epochs):
            model.train()
            train_loss = 0.0
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()

            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for images, labels in val_loader:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item()

            val_loss /= len(val_loader)
            results.append(val_loss)
            print(f'Fold {fold}, Epoch {epoch+1}, Training Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss}')

            torch.save(model.state_dict(), os.path.join(model_save_path, f'model_fold{fold}_epoch{epoch+1}.pth'))
        break  # fold 4에 대한 학습이 끝나면 반복문을 종료합니다.

average_val_loss = np.mean(results)
print(f'Average K-Fold Validation Loss: {average_val_loss}')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler
from torchvision import transforms
from PIL import Image
import os
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import random
import segmentation_models_pytorch as smp


def create_deeplabv3plus(num_classes):
    model = smp.DeepLabV3Plus(encoder_name="resnet101", classes=num_classes, encoder_weights="imagenet")
    return model

# 데이터셋 클래스 정의
class SegmentationDataset(Dataset):
    def __init__(self, image_dir, label_dir, image_list, transform=None):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.transform = transform
        self.images = image_list

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.images[idx])
        label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '_mask.png'))
        image = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')
        if self.transform:
            image = self.transform(image)
            label = self.transform(label) * 255
        return image, label.squeeze().long()

# 데이터 분할 함수
def split_dataset(image_dir, test_ratio=0.2):
    images = os.listdir(image_dir)
    random.seed(36)
    random.shuffle(images)
    split_idx = int(len(images) * (1 - test_ratio))
    return images[:split_idx], images[split_idx:]

# 데이터 변환 정의
train_transforms = transforms.Compose([
    transforms.Resize((640, 368)),
    transforms.ToTensor(),
])

test_transforms = transforms.Compose([
    transforms.Resize((640, 368)),
    transforms.ToTensor(),
])

# 데이터셋 경로 설정
image_dir = '/content/drive/MyDrive/횡단보도/segmentation/images'
label_dir = '/content/drive/MyDrive/횡단보도/segmentation/labels'

# 데이터셋 분할
train_images, test_images = split_dataset(image_dir)
train_dataset = SegmentationDataset(image_dir, label_dir, train_images, transform=train_transforms)
test_dataset = SegmentationDataset(image_dir, label_dir, test_images, transform=test_transforms)

# DataLoader 설정
test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False)

# 5폴드 교차검증 설정
k_folds = 5
kfold = KFold(n_splits=k_folds, shuffle=True)

# 폴드별 학습 및 검증
results = []

# 모델 저장 경로 설정
model_save_path = '/content/drive/MyDrive/횡단보도/segmentation/models'
if not os.path.exists(model_save_path):
    os.makedirs(model_save_path)

for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):
    print(f'Starting fold {fold}')
    train_subsampler = SubsetRandomSampler(train_ids)
    val_subsampler = SubsetRandomSampler(val_ids)

    train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=train_subsampler)
    val_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=val_subsampler)

    device =select_device('')
    model = create_deeplabv3plus(num_classes=4).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

def evaluate_model_on_test(model_path, test_loader, device):
    # 모델 구조 정의 (위에서 정의한 create_deeplabv3plus 함수 사용)
    model = create_deeplabv3plus(num_classes=4).to(device)

    # 저장된 모델 가중치 로드
    model.load_state_dict(torch.load(model_path))

    # 평가 모드 설정
    model.eval()

    # 정확도 계산을 위한 변수 초기화
    total_pixels = 0
    correct_pixels = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, dim=1)
            correct_pixels += (predicted == labels).sum().item()
            total_pixels += torch.numel(labels)


    # 정확도 출력
    accuracy = 100 * correct_pixels / total_pixels
    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')

# 모델 평가
model_path = '/content/drive/MyDrive/횡단보도/4models/model_fold4_epoch72.pth'  # 모델 경로
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
evaluate_model_on_test(model_path, test_loader, device)

clear_output()
print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))

!python /content/drive/MyDrive/yolov5/train.py --img 640 --batch 16 --epochs 30 --data data.yaml --weights yolov5m.pt --project /content/drive/MyDrive/yolov5/data/res

# Commented out IPython magic to ensure Python compatibility.

# 필요한 라이브러리 설치
!git clone https://github.com/ultralytics/ultralytics
# %cd /content/drive/MyDrive/yolov5/
!pip install -r /content/drive/MyDrive/yolov5/requirements.txt # 필요한 라이브러리 설치

import torch
from IPython.display import Image, clear_output  # 결과를 표시하기 위한 라이브러리
!python /content/drive/MyDrive/yolov5/train.py --img 640 --patience 40 --batch 16 --epochs 400 --data /content/drive/MyDrive/yolov5/data/data.yaml --weights yolov5l.pt

!python /content/drive/MyDrive/yolov5/val.py --img 640 --weights /content/drive/MyDrive/yolov5/runs/train/exp11/weights/best.pt --data /content/drive/MyDrive/yolov5/data/data.yaml --task test

# Commented out IPython magic to ensure Python compatibility.
# %cat /content/drive/MyDrive/yolov5/data/data.yaml

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ultralytics/yolov5  # YOLOv5 리포지토리 클론
# %cd yolov5

!pip install PyYAML
!pip install ultralytics
from ultralytics import YOLO

model = YOLO('yolov8l.pt')

# yolov8 학습
model.train(data='/content/drive/MyDrive/yolov5/data/data.yaml', epochs=100, batch= 16, imgsz=640)

!python /content/drive/MyDrive/yolov5/val.py --img 640 --weights /content/drive/MyDrive/yolov5/runs/train/exp5/weights/best.pt --task test

# Commented out IPython magic to ensure Python compatibility.

import torch
import os
from glob import glob
from PIL import Image
import numpy as np
from torchvision import transforms
!pip install -r /content/drive/MyDrive/yolov5/requirements.txt #경로수정
# %cd /content/drive/MyDrive/yolov5
from models.experimental import attempt_load  # YOLOv5 모델 로드 함수
from utils.general import non_max_suppression  # NMS 함수
from utils.torch_utils import select_device  # 디바이스 선택 함수
!pip install segmentation_models_pytorch
import segmentation_models_pytorch as smp #임포트 해야되고

def save_object_detection_results(pred, save_path, img_width, img_height):
    # 이미지 크기를 모델 입력 크기로 설정
    model_input_width, model_input_height = 352, 640
    result_lines = []
    for i, det in enumerate(pred):
        if len(det):
            for *xyxy, conf, cls in reversed(det):
                x_center = ((xyxy[0] + xyxy[2]) / 2) / model_input_width
                y_center = ((xyxy[1] + xyxy[3]) / 2) / model_input_height
                width = (xyxy[2] - xyxy[0]) / model_input_width
                height = (xyxy[3] - xyxy[1]) / model_input_height
                result_line = f"0 {x_center.item()} {y_center.item()} {width.item()} {height.item()}"
                result_lines.append(result_line)

    with open(save_path, 'w') as f:
        f.write('\n'.join(result_lines))

def predict_images_in_folder(image_folder, object_detection_model_path, segmentation_model_path, device, save_dir):
    object_detection_model = attempt_load(object_detection_model_path)  # 객체 탐지 모델 로드
    object_detection_model.to(device).eval()

    segmentation_model = smp.DeepLabV3Plus(  # 도로 영역 분류 모델 로드
        encoder_name="resnet101",
        encoder_weights=None,
        in_channels=3,
        classes=4
    )
    segmentation_model.load_state_dict(torch.load(segmentation_model_path, map_location=device))
    segmentation_model.to(device).eval()

    transform = transforms.Compose([  # 이미지 전처리
        transforms.Resize((640, 352)),
        transforms.ToTensor(),
    ])

    for image_path in glob(os.path.join(image_folder, '*.jpg')):
        image = Image.open(image_path).convert("RGB")
        img_width, img_height = image.size
        img_tensor = transform(image).unsqueeze(0).to(device)

        pred = object_detection_model(img_tensor, augment=False)[0]
        pred = non_max_suppression(pred, 0.3, 0.2, classes=None, agnostic=False)

        object_detection_save_path = os.path.join(save_dir, os.path.basename(image_path).replace('.jpg', '_detection.txt'))
        save_object_detection_results(pred, object_detection_save_path, img_width, img_height)

        seg_pred = segmentation_model(img_tensor)  # 도로 영역 분류 수행
        seg_pred = torch.argmax(seg_pred, dim=1).byte().cpu().numpy()

        road_area_mask = Image.fromarray(seg_pred[0])  # 도로 영역 분류 결과 저장
        road_area_mask_save_path = os.path.join(save_dir, os.path.basename(image_path).replace('.jpg', '_road_area_mask.png'))
        road_area_mask.save(road_area_mask_save_path)

# 예제 사용
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
image_folder = '/content/drive/MyDrive/yolov5/data/images/test'  # 분석할 이미지 폴더 경로
object_detection_model_path = '/content/drive/MyDrive/yolov5/runs/train/exp5/weights/best.pt'  # 객체 탐지 모델 파일 경로
segmentation_model_path = '/content/drive/MyDrive/횡단보도/segmentation/models/model_fold4_epoch33.pth'  # 도로 영역 분류 모델 파일 경로
save_dir = '/content/drive/MyDrive/yolo_1'  # 결과를 저장할 디렉토리 경로

predict_images_in_folder(image_folder, object_detection_model_path, segmentation_model_path, device, save_dir)

from PIL import Image, ImageDraw

def visualize_human_labels(human_label_file, output_file):
    # Load human object location labeling file
    with open(human_label_file, 'r') as f:
        human_label_data = f.readlines()

    # Create a white image
    img = Image.new('RGB', (640, 640), color='white')
    draw = ImageDraw.Draw(img)

    # Draw bounding boxes for human objects
    for label in human_label_data:
        label_data = label.strip().split()
        object_center_x, object_center_y, object_width, object_height = map(float, label_data[1:])
        x_min = object_center_x - object_width / 2
        y_min = object_center_y - object_height / 2
        x_max = object_center_x + object_width / 2
        y_max = object_center_y + object_height / 2

        # Draw bounding box of human object
        draw.rectangle([x_min * 640, y_min * 640, x_max * 640, y_max * 640], outline='red', width=2)

    # Save the image
    img.save(output_file)
    print("Human labels visualization saved at", output_file)

def visualize_road_labels(road_label_file, output_file):
    # Load road area detection labeling file
    road_label_img = Image.open(road_label_file)

    # Create a blank image to draw colored road areas
    colored_img = Image.new('RGB', road_label_img.size)
    draw = ImageDraw.Draw(colored_img)

    # Define colors for different road areas
    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Example colors: Red, Green, Blue

    # Highlight road areas with different colors
    for y in range(road_label_img.size[1]):
        for x in range(road_label_img.size[0]):
            pixel_color = road_label_img.getpixel((x, y))
            if pixel_color == (255, 255, 255):  # White color indicates road area
                # Choose a color based on the location or any other criteria
                color_index = (x + y) % len(colors)  # Example: Alternate colors based on x+y coordinate
                draw.point((x, y), fill=colors[color_index])

    # Save the image
    colored_img.save(output_file)
    print("Road labels visualization saved at", output_file)

# Example usage
human_label_file = '/content/240116173846-0043 O.txt'  # Example human object labeling file
#road_label_file = '/content/drive/MyDrive/sejong/test/segment_labels/0001.png'  # Example road area labeling file
output_human_image_file = '/content/1223.png'  # Output visualization image file path for human labels
#output_road_image_file = '/content/drive/MyDrive/sejong/test/001203123/road_labels.png'  # Output visualization image file path for road labels

visualize_human_labels(human_label_file, output_human_image_file)
#visualize_road_labels(road_label_file, output_road_image_file)
